name: Performance Testing

on:
    # Trigger on staging/production deployments
    workflow_run:
        workflows: ["Deploy to Staging", "Deploy to Production"]
        types:
            - completed
    # Manual workflow dispatch
    workflow_dispatch:
        inputs:
            environment:
                description: "Environment to test"
                required: true
                type: choice
                options:
                    - staging
                    - production
                default: staging
            url:
                description: "URL to test (optional, will use environment default if not provided)"
                required: false
                type: string
    # Weekly scheduled runs
    schedule:
        - cron: "0 6 * * 1" # Every Monday at 6am UTC

env:
    AWS_REGION: us-west-2

jobs:
    # Determine environment and URL
    setup:
        name: Setup Performance Test
        runs-on: ubuntu-latest
        outputs:
            environment: ${{ steps.determine-env.outputs.environment }}
            test-url: ${{ steps.determine-env.outputs.test-url }}
        steps:
            - name: Determine environment and URL
              id: determine-env
              run: |
                  # Determine environment based on trigger
                  if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
                    ENV="${{ inputs.environment }}"
                    URL="${{ inputs.url }}"
                  elif [ "${{ github.event_name }}" = "workflow_run" ]; then
                    # Extract environment from workflow name
                    if [[ "${{ github.event.workflow_run.name }}" == *"Staging"* ]]; then
                      ENV="staging"
                    elif [[ "${{ github.event.workflow_run.name }}" == *"Production"* ]]; then
                      ENV="production"
                    else
                      ENV="staging"
                    fi
                    URL=""
                  else
                    # Scheduled run - default to staging
                    ENV="staging"
                    URL=""
                  fi

                  # Set default URLs if not provided
                  if [ -z "$URL" ]; then
                    if [ "$ENV" = "production" ]; then
                      URL="https://bayoncoagent.com"
                    else
                      URL="https://staging.bayoncoagent.com"
                    fi
                  fi

                  echo "environment=$ENV" >> $GITHUB_OUTPUT
                  echo "test-url=$URL" >> $GITHUB_OUTPUT
                  echo "Testing environment: $ENV"
                  echo "Testing URL: $URL"

    # Run Lighthouse audits
    lighthouse-audit:
        name: Lighthouse Audit
        runs-on: ubuntu-latest
        needs: setup
        strategy:
            matrix:
                device: [desktop, mobile]
        steps:
            - name: Checkout code
              uses: actions/checkout@v4

            - name: Setup Node.js
              uses: actions/setup-node@v4
              with:
                  node-version: "20"
                  cache: "npm"

            - name: Install Lighthouse CLI
              run: npm install -g @lhci/cli@0.13.x lighthouse

            - name: Run Lighthouse audit (${{ matrix.device }})
              id: lighthouse
              run: |
                  # Configure device settings
                  if [ "${{ matrix.device }}" = "mobile" ]; then
                    PRESET="--preset=mobile"
                  else
                    PRESET="--preset=desktop"
                  fi

                  # Run Lighthouse
                  lighthouse "${{ needs.setup.outputs.test-url }}" \
                    $PRESET \
                    --output=html \
                    --output=json \
                    --output-path=./lighthouse-${{ matrix.device }} \
                    --chrome-flags="--headless --no-sandbox --disable-gpu" \
                    --quiet

                  # Extract scores from JSON report
                  PERFORMANCE=$(jq '.categories.performance.score * 100' lighthouse-${{ matrix.device }}.report.json)
                  ACCESSIBILITY=$(jq '.categories.accessibility.score * 100' lighthouse-${{ matrix.device }}.report.json)
                  BEST_PRACTICES=$(jq '.categories["best-practices"].score * 100' lighthouse-${{ matrix.device }}.report.json)
                  SEO=$(jq '.categories.seo.score * 100' lighthouse-${{ matrix.device }}.report.json)

                  echo "performance=$PERFORMANCE" >> $GITHUB_OUTPUT
                  echo "accessibility=$ACCESSIBILITY" >> $GITHUB_OUTPUT
                  echo "best-practices=$BEST_PRACTICES" >> $GITHUB_OUTPUT
                  echo "seo=$SEO" >> $GITHUB_OUTPUT

                  echo "## Lighthouse Scores (${{ matrix.device }})" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "| Category | Score |" >> $GITHUB_STEP_SUMMARY
                  echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
                  echo "| Performance | $PERFORMANCE |" >> $GITHUB_STEP_SUMMARY
                  echo "| Accessibility | $ACCESSIBILITY |" >> $GITHUB_STEP_SUMMARY
                  echo "| Best Practices | $BEST_PRACTICES |" >> $GITHUB_STEP_SUMMARY
                  echo "| SEO | $SEO |" >> $GITHUB_STEP_SUMMARY

            - name: Upload Lighthouse reports
              uses: actions/upload-artifact@v4
              with:
                  name: lighthouse-report-${{ matrix.device }}
                  path: |
                      lighthouse-${{ matrix.device }}.report.html
                      lighthouse-${{ matrix.device }}.report.json
                  retention-days: 30

            - name: Save scores for analysis
              run: |
                  # Create scores file for analysis job
                  cat > lighthouse-scores-${{ matrix.device }}.json << EOF
                  {
                    "device": "${{ matrix.device }}",
                    "performance": ${{ steps.lighthouse.outputs.performance }},
                    "accessibility": ${{ steps.lighthouse.outputs.accessibility }},
                    "bestPractices": ${{ steps.lighthouse.outputs.best-practices }},
                    "seo": ${{ steps.lighthouse.outputs.seo }},
                    "url": "${{ needs.setup.outputs.test-url }}",
                    "environment": "${{ needs.setup.outputs.environment }}",
                    "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
                    "commit": "${{ github.sha }}",
                    "workflow_run_id": "${{ github.run_id }}"
                  }
                  EOF

            - name: Upload scores artifact
              uses: actions/upload-artifact@v4
              with:
                  name: lighthouse-scores-${{ matrix.device }}
                  path: lighthouse-scores-${{ matrix.device }}.json
                  retention-days: 90

    # Analyze results against thresholds
    analyze-results:
        name: Analyze Results
        runs-on: ubuntu-latest
        needs: [setup, lighthouse-audit]
        outputs:
            passed: ${{ steps.check-thresholds.outputs.passed }}
            has-regressions: ${{ steps.check-regressions.outputs.has-regressions }}
        steps:
            - name: Checkout code
              uses: actions/checkout@v4

            - name: Download all score artifacts
              uses: actions/download-artifact@v4
              with:
                  pattern: lighthouse-scores-*
                  merge-multiple: true

            - name: Check scores against thresholds
              id: check-thresholds
              run: |
                  # Define thresholds
                  PERF_THRESHOLD=90
                  A11Y_THRESHOLD=95
                  BP_THRESHOLD=90
                  SEO_THRESHOLD=95

                  PASSED=true
                  FAILURES=""

                  # Check desktop scores
                  if [ -f "lighthouse-scores-desktop.json" ]; then
                    DESKTOP_PERF=$(jq -r '.performance' lighthouse-scores-desktop.json)
                    DESKTOP_A11Y=$(jq -r '.accessibility' lighthouse-scores-desktop.json)
                    DESKTOP_BP=$(jq -r '.bestPractices' lighthouse-scores-desktop.json)
                    DESKTOP_SEO=$(jq -r '.seo' lighthouse-scores-desktop.json)

                    echo "Desktop Scores:"
                    echo "  Performance: $DESKTOP_PERF (threshold: $PERF_THRESHOLD)"
                    echo "  Accessibility: $DESKTOP_A11Y (threshold: $A11Y_THRESHOLD)"
                    echo "  Best Practices: $DESKTOP_BP (threshold: $BP_THRESHOLD)"
                    echo "  SEO: $DESKTOP_SEO (threshold: $SEO_THRESHOLD)"

                    if (( $(echo "$DESKTOP_PERF < $PERF_THRESHOLD" | bc -l) )); then
                      PASSED=false
                      FAILURES="$FAILURES\n- Desktop Performance: $DESKTOP_PERF < $PERF_THRESHOLD"
                    fi
                    if (( $(echo "$DESKTOP_A11Y < $A11Y_THRESHOLD" | bc -l) )); then
                      PASSED=false
                      FAILURES="$FAILURES\n- Desktop Accessibility: $DESKTOP_A11Y < $A11Y_THRESHOLD"
                    fi
                    if (( $(echo "$DESKTOP_BP < $BP_THRESHOLD" | bc -l) )); then
                      PASSED=false
                      FAILURES="$FAILURES\n- Desktop Best Practices: $DESKTOP_BP < $BP_THRESHOLD"
                    fi
                    if (( $(echo "$DESKTOP_SEO < $SEO_THRESHOLD" | bc -l) )); then
                      PASSED=false
                      FAILURES="$FAILURES\n- Desktop SEO: $DESKTOP_SEO < $SEO_THRESHOLD"
                    fi
                  fi

                  # Check mobile scores
                  if [ -f "lighthouse-scores-mobile.json" ]; then
                    MOBILE_PERF=$(jq -r '.performance' lighthouse-scores-mobile.json)
                    MOBILE_A11Y=$(jq -r '.accessibility' lighthouse-scores-mobile.json)
                    MOBILE_BP=$(jq -r '.bestPractices' lighthouse-scores-mobile.json)
                    MOBILE_SEO=$(jq -r '.seo' lighthouse-scores-mobile.json)

                    echo "Mobile Scores:"
                    echo "  Performance: $MOBILE_PERF (threshold: $PERF_THRESHOLD)"
                    echo "  Accessibility: $MOBILE_A11Y (threshold: $A11Y_THRESHOLD)"
                    echo "  Best Practices: $MOBILE_BP (threshold: $BP_THRESHOLD)"
                    echo "  SEO: $MOBILE_SEO (threshold: $SEO_THRESHOLD)"

                    if (( $(echo "$MOBILE_PERF < $PERF_THRESHOLD" | bc -l) )); then
                      PASSED=false
                      FAILURES="$FAILURES\n- Mobile Performance: $MOBILE_PERF < $PERF_THRESHOLD"
                    fi
                    if (( $(echo "$MOBILE_A11Y < $A11Y_THRESHOLD" | bc -l) )); then
                      PASSED=false
                      FAILURES="$FAILURES\n- Mobile Accessibility: $MOBILE_A11Y < $A11Y_THRESHOLD"
                    fi
                    if (( $(echo "$MOBILE_BP < $BP_THRESHOLD" | bc -l) )); then
                      PASSED=false
                      FAILURES="$FAILURES\n- Mobile Best Practices: $MOBILE_BP < $BP_THRESHOLD"
                    fi
                    if (( $(echo "$MOBILE_SEO < $SEO_THRESHOLD" | bc -l) )); then
                      PASSED=false
                      FAILURES="$FAILURES\n- Mobile SEO: $MOBILE_SEO < $SEO_THRESHOLD"
                    fi
                  fi

                  echo "passed=$PASSED" >> $GITHUB_OUTPUT

                  if [ "$PASSED" = "false" ]; then
                    echo "## ❌ Performance Thresholds Not Met" >> $GITHUB_STEP_SUMMARY
                    echo "" >> $GITHUB_STEP_SUMMARY
                    echo "The following scores are below thresholds:" >> $GITHUB_STEP_SUMMARY
                    echo -e "$FAILURES" >> $GITHUB_STEP_SUMMARY
                    echo "" >> $GITHUB_STEP_SUMMARY
                    echo "Review the Lighthouse reports for detailed recommendations." >> $GITHUB_STEP_SUMMARY
                  else
                    echo "## ✅ All Performance Thresholds Met" >> $GITHUB_STEP_SUMMARY
                    echo "" >> $GITHUB_STEP_SUMMARY
                    echo "All Lighthouse scores meet or exceed the defined thresholds." >> $GITHUB_STEP_SUMMARY
                  fi

            - name: Check for performance regressions
              id: check-regressions
              run: |
                  # This would compare against historical baseline
                  # For now, we'll create a placeholder that stores the current scores as baseline

                  HAS_REGRESSIONS=false

                  # In a real implementation, this would:
                  # 1. Fetch historical performance data from a database or artifact storage
                  # 2. Compare current scores against the baseline
                  # 3. Identify regressions (>10% decrease in any score)
                  # 4. Set HAS_REGRESSIONS=true if regressions found

                  echo "has-regressions=$HAS_REGRESSIONS" >> $GITHUB_OUTPUT

                  echo "## Performance Regression Check" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "ℹ️ Historical baseline comparison not yet implemented." >> $GITHUB_STEP_SUMMARY
                  echo "Current scores will be stored as baseline for future comparisons." >> $GITHUB_STEP_SUMMARY

            - name: Fail if thresholds not met
              if: steps.check-thresholds.outputs.passed == 'false'
              run: |
                  echo "❌ Performance tests failed - scores below thresholds"
                  exit 1

    # Generate and store performance reports
    report-results:
        name: Report Results
        runs-on: ubuntu-latest
        needs: [setup, lighthouse-audit, analyze-results]
        if: always()
        steps:
            - name: Checkout code
              uses: actions/checkout@v4

            - name: Download all artifacts
              uses: actions/download-artifact@v4
              with:
                  pattern: lighthouse-*
                  merge-multiple: true

            - name: Generate performance summary
              id: summary
              run: |
                  # Create comprehensive performance report
                  cat > performance-report.md << 'EOF'
                  # Performance Test Report

                  **Environment:** ${{ needs.setup.outputs.environment }}
                  **URL:** ${{ needs.setup.outputs.test-url }}
                  **Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
                  **Commit:** ${{ github.sha }}
                  **Workflow Run:** ${{ github.run_id }}

                  ## Lighthouse Scores

                  ### Desktop
                  EOF

                  if [ -f "lighthouse-scores-desktop.json" ]; then
                    DESKTOP_PERF=$(jq -r '.performance' lighthouse-scores-desktop.json)
                    DESKTOP_A11Y=$(jq -r '.accessibility' lighthouse-scores-desktop.json)
                    DESKTOP_BP=$(jq -r '.bestPractices' lighthouse-scores-desktop.json)
                    DESKTOP_SEO=$(jq -r '.seo' lighthouse-scores-desktop.json)

                    cat >> performance-report.md << EOF

                  | Category | Score | Status |
                  |----------|-------|--------|
                  | Performance | $DESKTOP_PERF | $([ $(echo "$DESKTOP_PERF >= 90" | bc -l) -eq 1 ] && echo "✅" || echo "❌") |
                  | Accessibility | $DESKTOP_A11Y | $([ $(echo "$DESKTOP_A11Y >= 95" | bc -l) -eq 1 ] && echo "✅" || echo "❌") |
                  | Best Practices | $DESKTOP_BP | $([ $(echo "$DESKTOP_BP >= 90" | bc -l) -eq 1 ] && echo "✅" || echo "❌") |
                  | SEO | $DESKTOP_SEO | $([ $(echo "$DESKTOP_SEO >= 95" | bc -l) -eq 1 ] && echo "✅" || echo "❌") |
                  EOF
                  fi

                  cat >> performance-report.md << 'EOF'

                  ### Mobile
                  EOF

                  if [ -f "lighthouse-scores-mobile.json" ]; then
                    MOBILE_PERF=$(jq -r '.performance' lighthouse-scores-mobile.json)
                    MOBILE_A11Y=$(jq -r '.accessibility' lighthouse-scores-mobile.json)
                    MOBILE_BP=$(jq -r '.bestPractices' lighthouse-scores-mobile.json)
                    MOBILE_SEO=$(jq -r '.seo' lighthouse-scores-mobile.json)

                    cat >> performance-report.md << EOF

                  | Category | Score | Status |
                  |----------|-------|--------|
                  | Performance | $MOBILE_PERF | $([ $(echo "$MOBILE_PERF >= 90" | bc -l) -eq 1 ] && echo "✅" || echo "❌") |
                  | Accessibility | $MOBILE_A11Y | $([ $(echo "$MOBILE_A11Y >= 95" | bc -l) -eq 1 ] && echo "✅" || echo "❌") |
                  | Best Practices | $MOBILE_BP | $([ $(echo "$MOBILE_BP >= 90" | bc -l) -eq 1 ] && echo "✅" || echo "❌") |
                  | SEO | $MOBILE_SEO | $([ $(echo "$MOBILE_SEO >= 95" | bc -l) -eq 1 ] && echo "✅" || echo "❌") |
                  EOF
                  fi

                  cat >> performance-report.md << 'EOF'

                  ## Thresholds

                  - Performance: ≥ 90
                  - Accessibility: ≥ 95
                  - Best Practices: ≥ 90
                  - SEO: ≥ 95

                  ## Reports

                  Detailed Lighthouse reports are available as workflow artifacts:
                  - Desktop Report (HTML)
                  - Mobile Report (HTML)

                  EOF

                  cat performance-report.md

            - name: Upload performance report
              uses: actions/upload-artifact@v4
              with:
                  name: performance-report
                  path: performance-report.md
                  retention-days: 90

            - name: Store results in performance database
              id: store-results
              run: |
                  # This would store results in a database for trend analysis
                  # For now, we'll just create a JSON file with all the data

                  cat > performance-data.json << EOF
                  {
                    "environment": "${{ needs.setup.outputs.environment }}",
                    "url": "${{ needs.setup.outputs.test-url }}",
                    "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
                    "commit": "${{ github.sha }}",
                    "workflow_run_id": "${{ github.run_id }}",
                    "passed": ${{ needs.analyze-results.outputs.passed }},
                    "has_regressions": ${{ needs.analyze-results.outputs.has-regressions }},
                    "scores": {
                      "desktop": $(cat lighthouse-scores-desktop.json 2>/dev/null || echo "null"),
                      "mobile": $(cat lighthouse-scores-mobile.json 2>/dev/null || echo "null")
                    }
                  }
                  EOF

                  echo "Performance data stored for trend analysis"
                  cat performance-data.json

            - name: Upload performance data
              uses: actions/upload-artifact@v4
              with:
                  name: performance-data
                  path: performance-data.json
                  retention-days: 365

            - name: Generate trend charts
              run: |
                  # Placeholder for trend chart generation
                  # In a real implementation, this would:
                  # 1. Fetch historical performance data
                  # 2. Generate charts showing score trends over time
                  # 3. Identify patterns and anomalies

                  echo "## Performance Trends" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "ℹ️ Trend chart generation not yet implemented." >> $GITHUB_STEP_SUMMARY
                  echo "Historical data is being collected for future trend analysis." >> $GITHUB_STEP_SUMMARY

            - name: Post summary to workflow
              run: |
                  cat performance-report.md >> $GITHUB_STEP_SUMMARY

    # Send notifications
    notify:
        name: Send Notifications
        runs-on: ubuntu-latest
        needs: [setup, analyze-results, report-results]
        if: always()
        steps:
            - name: Checkout code
              uses: actions/checkout@v4

            - name: Download performance report
              uses: actions/download-artifact@v4
              with:
                  name: performance-report

            - name: Notify success
              if: needs.analyze-results.outputs.passed == 'true'
              uses: ./.github/actions/slack-notify
              with:
                  webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
                  message-type: success
                  title: "✅ Performance Tests Passed"
                  message: |
                      Performance tests completed successfully for ${{ needs.setup.outputs.environment }} environment.

                      All Lighthouse scores meet or exceed thresholds.

                      *URL:* ${{ needs.setup.outputs.test-url }}
                      *Commit:* ${{ github.sha }}

                      View detailed reports in workflow artifacts.
                  environment: ${{ needs.setup.outputs.environment }}
                  commit-sha: ${{ github.sha }}
                  author: ${{ github.actor }}

            - name: Notify failure
              if: needs.analyze-results.outputs.passed == 'false'
              uses: ./.github/actions/slack-notify
              with:
                  webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
                  message-type: error
                  title: "❌ Performance Tests Failed"
                  message: |
                      Performance tests failed for ${{ needs.setup.outputs.environment }} environment.

                      One or more Lighthouse scores are below thresholds.

                      *URL:* ${{ needs.setup.outputs.test-url }}
                      *Commit:* ${{ github.sha }}

                      Review the workflow logs and Lighthouse reports for details.
                  environment: ${{ needs.setup.outputs.environment }}
                  commit-sha: ${{ github.sha }}
                  author: ${{ github.actor }}
                  mention-users: ${{ secrets.SLACK_DEVOPS_USERS }}

            - name: Notify regressions
              if: needs.analyze-results.outputs.has-regressions == 'true'
              uses: ./.github/actions/slack-notify
              with:
                  webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
                  message-type: warning
                  title: "⚠️ Performance Regression Detected"
                  message: |
                      Performance regression detected in ${{ needs.setup.outputs.environment }} environment.

                      One or more scores decreased by more than 10% compared to baseline.

                      *URL:* ${{ needs.setup.outputs.test-url }}
                      *Commit:* ${{ github.sha }}

                      Review the performance report for details.
                  environment: ${{ needs.setup.outputs.environment }}
                  commit-sha: ${{ github.sha }}
                  author: ${{ github.actor }}
                  mention-users: ${{ secrets.SLACK_DEVOPS_USERS }}
